{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from utilities import *\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = .15\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"smadc\": get_SMADC_folder_data(),\n",
    "    \"annotated\": get_annotated_data_folder_data(),\n",
    "    \"dart\": get_dart_folder_data(),\n",
    "    \"aoc\": get_arabic_dialects_dataset_folder_data(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dialects\n",
    "lx = {\n",
    "    \"EGY_EMOTIONS\": None,\n",
    "    \"GLF_EMOTIONS\": None,\n",
    "    \"EGY_UNKOWN\": None,\n",
    "    \"GLF_UNKOWN\": None,\n",
    "}\n",
    "for path in glob(\"counter_lexicon/*\"):\n",
    "    name = Path(path).stem\n",
    "    lx[name] = pd.read_csv(f\"counter_lexicon/{name}.csv\")[[\"Text\", \"Emotion\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "lx[\"EGY_EMOTIONS\"], EGY_EMOTIONS_VALIDATION = train_test_split(lx[\"EGY_EMOTIONS\"], test_size=.1, random_state=seed, shuffle=True)\n",
    "lx[\"GLF_EMOTIONS\"], GLF_EMOTIONS_VALIDATION = train_test_split(lx[\"GLF_EMOTIONS\"], test_size=.1, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classifier(\n",
    "    df: pd.DataFrame,\n",
    "    test_size: float=test_size,\n",
    "    seed: int=seed\n",
    "    ) -> Pipeline:\n",
    "\n",
    "    train, test = train_test_split(df, test_size=test_size, random_state=seed, shuffle=True)\n",
    "    train_x, train_y = train[\"Text\"], train[\"Emotion\"]\n",
    "    test_x, test_y = test[\"Text\"], test[\"Emotion\"]\n",
    "    clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('clf', MultinomialNB()),\n",
    "    ])\n",
    "\n",
    "    clf.fit(train_x, train_y)\n",
    "\n",
    "    # Report\n",
    "    preds = clf.predict(test_x)\n",
    "    print(classification_report(test_y, preds, digits=4))\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Classifier scores alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER     0.8843    0.6903    0.7754       155\n",
      "     DISGUST     1.0000    0.3793    0.5500        29\n",
      "        FEAR     1.0000    0.4194    0.5909        31\n",
      "         JOY     0.8827    0.9251    0.9034       187\n",
      "     SADNESS     0.7985    0.8359    0.8168       256\n",
      "    SURPRISE     0.5920    0.8240    0.6890       125\n",
      "\n",
      "    accuracy                         0.7931       783\n",
      "   macro avg     0.8596    0.6790    0.7209       783\n",
      "weighted avg     0.8181    0.7931    0.7900       783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GLF_clf = generate_classifier(lx[\"GLF_EMOTIONS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER     0.9819    0.9958    0.9888      2611\n",
      "     DISGUST     0.9529    0.8100    0.8757       100\n",
      "        FEAR     0.0000    0.0000    0.0000         4\n",
      "         JOY     1.0000    0.7750    0.8732        40\n",
      "     SADNESS     0.8714    0.8243    0.8472        74\n",
      "    SURPRISE     0.0000    0.0000    0.0000         5\n",
      "\n",
      "    accuracy                         0.9785      2834\n",
      "   macro avg     0.6344    0.5675    0.5975      2834\n",
      "weighted avg     0.9751    0.9785    0.9763      2834\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwarez\\Anaconda\\envs\\emotion_research\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Softwarez\\Anaconda\\envs\\emotion_research\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Softwarez\\Anaconda\\envs\\emotion_research\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "EGY_clf = generate_classifier(lx[\"EGY_EMOTIONS\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Classifier scores combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ANGER     0.9585    0.9806    0.9694      2732\n",
      "     DISGUST     0.9851    0.5690    0.7213       116\n",
      "        FEAR     1.0000    0.1667    0.2857        36\n",
      "         JOY     0.9170    0.8855    0.9010       262\n",
      "     SADNESS     0.8416    0.8018    0.8212       338\n",
      "    SURPRISE     0.5460    0.7143    0.6189       133\n",
      "\n",
      "    accuracy                         0.9259      3617\n",
      "   macro avg     0.8747    0.6863    0.7196      3617\n",
      "weighted avg     0.9307    0.9259    0.9230      3617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df = pd.concat([lx[\"GLF_EMOTIONS\"], lx[\"EGY_EMOTIONS\"]])\n",
    "general_clf = generate_classifier(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General emotion classifier Vs. GLF specific classifier on validation set:\n",
      "    \tgeneral_score: 0.18007662835249041\n",
      "    \tGLF_score: 0.3371647509578544\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "general_score = general_clf.score(\n",
    "    general_clf.predict(GLF_EMOTIONS_VALIDATION[\"Text\"]),\n",
    "    GLF_EMOTIONS_VALIDATION[\"Emotion\"]\n",
    ")\n",
    "\n",
    "GLF_score = GLF_clf.score(\n",
    "    GLF_clf.predict(GLF_EMOTIONS_VALIDATION[\"Text\"]),\n",
    "    GLF_EMOTIONS_VALIDATION[\"Emotion\"]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\"\"General emotion classifier Vs. GLF specific classifier on validation set:\n",
    "    \\tgeneral_score: {general_score}\n",
    "    \\tGLF_score: {GLF_score}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General emotion classifier Vs. EGY specific classifier on validation set:\n",
      "    \tgeneral_score: 0.917989417989418\n",
      "    \tEGY_score: 0.917989417989418\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "general_score = general_clf.score(\n",
    "    general_clf.predict(EGY_EMOTIONS_VALIDATION[\"Text\"]),\n",
    "    EGY_EMOTIONS_VALIDATION[\"Emotion\"]\n",
    ")\n",
    "\n",
    "EGY_score = EGY_clf.score(\n",
    "    EGY_clf.predict(EGY_EMOTIONS_VALIDATION[\"Text\"]),\n",
    "    EGY_EMOTIONS_VALIDATION[\"Emotion\"]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\"\"General emotion classifier Vs. EGY specific classifier on validation set:\n",
    "    \\tgeneral_score: {general_score}\n",
    "    \\tEGY_score: {EGY_score}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set CSV generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_df(\n",
    "    dataset: pd.DataFrame,\n",
    "    clf: MultinomialNB\n",
    "):\n",
    "    cols = [\n",
    "        \"Original text\",\n",
    "        \"True label\",\n",
    "        \"Predicted label\",\n",
    "        *[col +  \" probability\" for col in clf.classes_],\n",
    "    ]\n",
    "\n",
    "    output_df = pd.DataFrame(columns=cols)\n",
    "    for i, (t, tl, pl, prob) in enumerate(zip(\n",
    "        dataset[\"Text\"],\n",
    "        dataset[\"Emotion\"],\n",
    "        clf.predict(dataset[\"Text\"]),\n",
    "        clf.predict_proba(dataset[\"Text\"])\n",
    "    )):\n",
    "        output_df.loc[i] = [t, tl, pl, *prob]\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_output_df(GLF_EMOTIONS_VALIDATION, GLF_clf).to_csv(\"output_csvs/GLF_clf__GLF_validation.csv\")\n",
    "generate_output_df(GLF_EMOTIONS_VALIDATION, general_clf).to_csv(\"output_csvs/general_clf__GLF_validation.csv\")\n",
    "generate_output_df(EGY_EMOTIONS_VALIDATION, EGY_clf).to_csv(\"output_csvs/EGY_clf__EGY_validation.csv\")\n",
    "generate_output_df(EGY_EMOTIONS_VALIDATION, general_clf).to_csv(\"output_csvs/general_clf__EGY_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b146cda2b5e37bb330cb0c5ca2b4b0706c5eef902c6118ca3e1c5f676b50d986"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('emotion_research')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
